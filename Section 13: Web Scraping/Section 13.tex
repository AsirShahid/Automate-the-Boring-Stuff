% Created 2021-08-05 Thu 18:56
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{Mohammed Asir Shahid}
\date{2021-08-05}
\title{Section 13\\\medskip
\large Web Scraping}
\hypersetup{
 pdfauthor={Mohammed Asir Shahid},
 pdftitle={Section 13},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 27.2 (Org mode 9.5)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents


\section{The webbrowser Module}
\label{sec:org6aabdf9}

\begin{verbatim}

import webbrowser

webbrowser.open("https://asir.dev")

\end{verbatim}

Let's create a program that can open a given address on maps.

\begin{verbatim}

import webbrowser, sys, pyperclip

sys.argv # ["mapit.py", "870", "Valencia", "St."]

# Check if command line arguments were passed

if len(sys.argv) > 1:
    # ["mapit.py", "870", "Valencia", "St."] -> 870 Valencia St.
    address=" ".join(sys.argv[1:])
else:
    address=pyperclip.paste()

webbrowser.open("https://www.google.com/maps/place/%s" % (address))

\end{verbatim}

\section{Downloading from the Web with the Requests Module}
\label{sec:orgf91dd10}

The requests module lets you easily download files from the web without having to worry about complicated network issues. The requests module is a third party module which we'll need to install on our own.

\begin{verbatim}
pip install requests
\end{verbatim}

We can pass a URL to the requests.get() function in order to get the file. We can check the status code to see if it downloaded properly, if so then we'll get the status code 200. We can print out the file using .text. We can also see if there is an issue by calling the raise\textsubscript{for}\textsubscript{status}() method which will raise an error if we ran into any problems.

\begin{verbatim}

import requests

res=requests.get("http://automatetheboringstuff.com/files/rj.txt")

print(res.status_code)

print(len(res.text))
print(res.text[:500])

print(res.raise_for_status())

badRes=requests.get("http://automatetheboringstuff.com/files/rjuliet.txt")

print(badRes.raise_for_status())

\end{verbatim}

\subsection{Write-binary mode: open(filename, ``wb'')}
\label{sec:orgd644273}

We can save a web page to a file using the open function. However, we must do somethings differently.

\begin{verbatim}

import requests

res=requests.get("http://automatetheboringstuff.com/files/rj.txt")
playFile=open("RomeoAndJuliet.txt","wb")

for chunk in res.iter_content(100000):
    playFile.write(chunk)

playFile.close()

\end{verbatim}

Request module functions can be useful, but they are somewhat limited. You can only use it when you have the exact URL that you need to download. Selenium lets your Python scripts control the web browser directly.
\section{Parsing HTML with the Beautiful Soup Module}
\label{sec:org5f1b5c3}

Here we will learn how to write programs that pull information off of web pages. This is known as web scraping. We have a third party module called beautifulsoup which makes parsing through websites HTML much easier.

\begin{verbatim}
pip install beautifulsoup4
\end{verbatim}

\begin{verbatim}

import bs4

\end{verbatim}

Let's try to parse through an eBay page and scrape the price information from that page.

\begin{verbatim}

import bs4,requests

url="https://www.ebay.com/itm/313628358864"

def geteBayPrice(productURL):
    response = requests.get(url)

    response.raise_for_status()

    soup=bs4.BeautifulSoup(response.text,"html.parser")

    elems=soup.select("#prcIsum")

    return elems[0].text.strip("US").strip()

url="https://www.ebay.com/itm/313628358864"

price=geteBayPrice(url)

print("The price is %s" % (price))

\end{verbatim}

\begin{verbatim}
The price is $20.51
\end{verbatim}


This code is pretty reliant on the website's CSS staying the same.

\begin{verbatim}

import bs4,requests


def getTemperature(url):
    response = requests.get(url)

    response.raise_for_status()

    soup=bs4.BeautifulSoup(response.text,"html.parser")

    elems=soup.select("#current_conditions-summary > p.myforecast-current-lrg")


    return elems[0].text.strip()

url="https://forecast.weather.gov/MapClick.php?lat=40.8667&lon=-73.0343"



temp=getTemperature(url)

print("The temperature is %s" % (temp))

\end{verbatim}

\begin{verbatim}
The temperature is 76Â°F
\end{verbatim}
\section{Controlling the Browser with the Selenium Module}
\label{sec:orgf2b6837}

We have learned how to download webpages and parse their HTML. However, sometimes the webpages we want to access require logging in or relying on Javascript. In these cases, we can use Selenium which will open a programatically controlled webbrowser. It's a third party module that we need to install ourselves.

\begin{verbatim}
pip install selenium
\end{verbatim}

\begin{verbatim}

from selenium import webdriver

browser=webdriver.Firefox()

browser.get("https://automatetheboringstuff.com")

elem=browser.find_element_by_css_selector(".main > div:nth-child(1) > ul:nth-child(21) > li:nth-child(1) > a:nth-child(1)")

elem.click()

elems=browser.find_elements_by_css_selector('p')
print(len(elems))

\end{verbatim}

\begin{verbatim}
109
\end{verbatim}


Selenium has several methods of getting web elements from the page. The most commonly used ones are find\textsubscript{element}\textsubscript{by}\textsubscript{css}\textsubscript{selector} and find\textsubscript{elements}\textsubscript{by}\textsubscript{css}\textsubscript{selector}. However, we can also find by class name, id, link text, partial text, and tag name.

\begin{verbatim}

from selenium import webdriver

browser=webdriver.Firefox()

browser.get("https://asir.dev")

postElem=browser.find_element_by_css_selector("#blog-link")

postElem.click()

browser.back()
browser.forward()
browser.refresh()
#browser.quit()

searchElem=browser.find_element_by_css_selector("#search-box")

search="Post"

searchElem.send_keys(search)
searchElem.submit()

\end{verbatim}

Now we can take a look at how Python scripts can use Selenium to read webpages.

\begin{verbatim}

from selenium import webdriver

browser=webdriver.Firefox()

browser.get("https://automatetheboringstuff.com")
elem=browser.find_element_by_css_selector(".main > div:nth-child(1) > blockquote:nth-child(6)")
print(elem.text)

elem=browser.find_element_by_css_selector("html")
print(elem.text)


\end{verbatim}
\end{document}
